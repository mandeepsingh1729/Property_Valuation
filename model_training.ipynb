{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "jKbmySZO8U9_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa-v5wEi8YoW",
        "outputId": "4f213400-0236-405d-a4d8-282633ce18cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationFunction:\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return 1 - x**2\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "ciqmtdtSbe3i"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self, layer_sizes, activation='sigmoid'):\n",
        "\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation_name = activation\n",
        "        self.L = len(layer_sizes) - 1\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(self.L):\n",
        "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0/layer_sizes[i]))\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
        "\n",
        "        self.velocity_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.velocity_b = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        self.activations = [X]\n",
        "        self.zs = []\n",
        "\n",
        "        for i in range(self.L - 1):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.zs.append(z)\n",
        "\n",
        "            if self.activation_name == 'sigmoid':\n",
        "                a = ActivationFunction.sigmoid(z)\n",
        "            elif self.activation_name == 'relu':\n",
        "                a = ActivationFunction.relu(z)\n",
        "            elif self.activation_name == 'tanh':\n",
        "                a = ActivationFunction.tanh(z)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown activation: {self.activation_name}\")\n",
        "\n",
        "            self.activations.append(a)\n",
        "\n",
        "        if self.L > 0:\n",
        "            z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "            self.zs.append(z)\n",
        "            a = ActivationFunction.softmax(z)\n",
        "            self.activations.append(a)\n",
        "\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def backward(self, X, y, learning_rate, momentum=0.0):\n",
        "\n",
        "        m = X.shape[0]\n",
        "\n",
        "        y_one_hot = np.zeros((m, self.layer_sizes[-1]))\n",
        "        y_one_hot[np.arange(m), y.astype(int)] = 1\n",
        "\n",
        "        predictions = self.forward(X)\n",
        "\n",
        "        dW = [np.zeros_like(w) for w in self.weights]\n",
        "        db = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        error = predictions - y_one_hot\n",
        "\n",
        "        for l in reversed(range(self.L)):\n",
        "            if l == self.L - 1:\n",
        "                dW[l] = np.dot(self.activations[l].T, error) / m\n",
        "                db[l] = np.sum(error, axis=0, keepdims=True) / m\n",
        "            else:\n",
        "                if self.activation_name == 'sigmoid':\n",
        "                    activation_derivative = ActivationFunction.sigmoid_derivative(self.activations[l+1])\n",
        "                elif self.activation_name == 'relu':\n",
        "                    activation_derivative = ActivationFunction.relu_derivative(self.activations[l+1])\n",
        "                elif self.activation_name == 'tanh':\n",
        "                    activation_derivative = ActivationFunction.tanh_derivative(self.activations[l+1])\n",
        "\n",
        "                error = np.dot(error, self.weights[l+1].T) * activation_derivative\n",
        "                dW[l] = np.dot(self.activations[l].T, error) / m\n",
        "                db[l] = np.sum(error, axis=0, keepdims=True) / m\n",
        "\n",
        "        for l in range(self.L):\n",
        "            self.velocity_w[l] = momentum * self.velocity_w[l] + learning_rate * dW[l]\n",
        "            self.velocity_b[l] = momentum * self.velocity_b[l] + learning_rate * db[l]\n",
        "            self.weights[l] -= self.velocity_w[l]\n",
        "            self.biases[l] -= self.velocity_b[l]\n",
        "\n",
        "        return self.weights, self.biases\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        predictions = self.forward(X)\n",
        "        return np.argmax(predictions, axis=1)\n",
        "\n",
        "    def accuracy(self, X, y):\n",
        "\n",
        "        predictions = self.predict(X)\n",
        "        return np.mean(predictions == y)\n"
      ],
      "metadata": {
        "id": "YsY4tbyRbi0q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "\n",
        "        next(reader)\n",
        "\n",
        "        for row in reader:\n",
        "            values = np.array(row, dtype=float)\n",
        "            labels.append(values[0])\n",
        "            data.append(values[1:])\n",
        "\n",
        "    x = np.array(data, dtype=np.float32) / 255.0\n",
        "    y = np.array(labels, dtype=np.int64)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "j-NsXshlb2aE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/mnist_train.csv\"\n",
        "\n",
        "\n",
        "x, y = load_data(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "cJ8GmGL2cD2V"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop_single_layer(x, y, weights, biases, learning_rate):\n",
        "\n",
        "    m = x.shape[0]\n",
        "    num_classes = weights[0].shape[1]\n",
        "\n",
        "    y_one_hot = np.zeros((m, num_classes))\n",
        "    y_one_hot[np.arange(m), y.astype(int)] = 1\n",
        "\n",
        "\n",
        "    z = np.dot(x, weights[0]) + biases[0]\n",
        "\n",
        "\n",
        "    predictions = ActivationFunction.softmax(z)\n",
        "\n",
        "    loss = -np.sum(y_one_hot * np.log(predictions + 1e-8)) / m\n",
        "\n",
        "\n",
        "    error = predictions - y_one_hot\n",
        "\n",
        "\n",
        "    dW = np.dot(x.T, error) / m\n",
        "\n",
        "\n",
        "    db = np.sum(error, axis=0, keepdims=True) / m\n",
        "\n",
        "\n",
        "    weights[0] -= learning_rate * dW\n",
        "    biases[0] -= learning_rate * db\n",
        "\n",
        "    return weights, biases, loss"
      ],
      "metadata": {
        "id": "zXLwCs2JcN5T"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop_two_layer(x, y, weights, biases, learning_rate):\n",
        "\n",
        "    m = x.shape[0]\n",
        "\n",
        "    y_one_hot = np.zeros((m, 10))\n",
        "    y_one_hot[np.arange(m), y.astype(int)] = 1\n",
        "\n",
        "\n",
        "    z1 = np.dot(x, weights[0]) + biases[0]\n",
        "    a1 = ActivationFunction.sigmoid(z1)\n",
        "\n",
        "    z2 = np.dot(a1, weights[1]) + biases[1]\n",
        "    predictions = ActivationFunction.softmax(z2)\n",
        "\n",
        "    loss = -np.sum(y_one_hot * np.log(predictions + 1e-8)) / m\n",
        "\n",
        "\n",
        "    error2 = predictions - y_one_hot\n",
        "    dW2 = np.dot(a1.T, error2) / m\n",
        "    db2 = np.sum(error2, axis=0, keepdims=True) / m\n",
        "\n",
        "\n",
        "    error1 = np.dot(error2, weights[1].T) * ActivationFunction.sigmoid_derivative(a1)\n",
        "    dW1 = np.dot(x.T, error1) / m\n",
        "    db1 = np.sum(error1, axis=0, keepdims=True) / m\n",
        "\n",
        "    weights[0] -= learning_rate * dW1\n",
        "    weights[1] -= learning_rate * dW2\n",
        "    biases[0] -= learning_rate * db1\n",
        "    biases[1] -= learning_rate * db2\n",
        "\n",
        "    return weights, biases, loss\n",
        "\n"
      ],
      "metadata": {
        "id": "RXn1rZ87chCQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop_multi_layer(x, y, weights, biases, learning_rate):\n",
        "\n",
        "    m = x.shape[0]\n",
        "    num_layers = len(weights)\n",
        "    num_classes = weights[-1].shape[1]\n",
        "\n",
        "    y_one_hot = np.zeros((m, num_classes))\n",
        "    y_one_hot[np.arange(m), y.astype(int)] = 1\n",
        "\n",
        "    activations = [x]\n",
        "    zs = []\n",
        "\n",
        "    for l in range(num_layers - 1):\n",
        "        z = np.dot(activations[-1], weights[l]) + biases[l]\n",
        "        zs.append(z)\n",
        "        a = ActivationFunction.sigmoid(z)\n",
        "        activations.append(a)\n",
        "\n",
        "    z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
        "    zs.append(z)\n",
        "    predictions = ActivationFunction.softmax(z)\n",
        "    activations.append(predictions)\n",
        "\n",
        "    loss = -np.sum(y_one_hot * np.log(predictions + 1e-8)) / m\n",
        "\n",
        "    dW = [np.zeros_like(w) for w in weights]\n",
        "    db = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    error = predictions - y_one_hot\n",
        "    dW[-1] = np.dot(activations[-2].T, error) / m\n",
        "    db[-1] = np.sum(error, axis=0, keepdims=True) / m\n",
        "\n",
        "    for l in reversed(range(num_layers - 1)):\n",
        "        error = np.dot(error, weights[l+1].T) * ActivationFunction.sigmoid_derivative(activations[l+1])\n",
        "        dW[l] = np.dot(activations[l].T, error) / m\n",
        "        db[l] = np.sum(error, axis=0, keepdims=True) / m\n",
        "\n",
        "    for l in range(num_layers):\n",
        "        weights[l] -= learning_rate * dW[l]\n",
        "        biases[l] -= learning_rate * db[l]\n",
        "\n",
        "    return weights, biases, loss"
      ],
      "metadata": {
        "id": "SwUFDoQgt5Fp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop_multi_layer_activation(x, y, weights, biases, learning_rate, activation='sigmoid'):\n",
        "\n",
        "    m = x.shape[0]\n",
        "    num_layers = len(weights)\n",
        "    num_classes = weights[-1].shape[1]\n",
        "\n",
        "    y_one_hot = np.zeros((m, num_classes))\n",
        "    y_one_hot[np.arange(m), y.astype(int)] = 1\n",
        "\n",
        "    if activation == 'sigmoid':\n",
        "        activation_func = ActivationFunction.sigmoid\n",
        "        activation_derivative = ActivationFunction.sigmoid_derivative\n",
        "    elif activation == 'relu':\n",
        "        activation_func = ActivationFunction.relu\n",
        "        activation_derivative = ActivationFunction.relu_derivative\n",
        "    elif activation == 'tanh':\n",
        "        activation_func = ActivationFunction.tanh\n",
        "        activation_derivative = ActivationFunction.tanh_derivative\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported activation: {activation}\")\n",
        "\n",
        "    activations = [x]\n",
        "    zs = []\n",
        "\n",
        "    for l in range(num_layers - 1):\n",
        "        z = np.dot(activations[-1], weights[l]) + biases[l]\n",
        "        zs.append(z)\n",
        "        a = activation_func(z)\n",
        "        activations.append(a)\n",
        "\n",
        "    z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
        "    zs.append(z)\n",
        "    predictions = ActivationFunction.softmax(z)\n",
        "    activations.append(predictions)\n",
        "\n",
        "    loss = -np.sum(y_one_hot * np.log(predictions + 1e-8)) / m\n",
        "\n",
        "    dW = [np.zeros_like(w) for w in weights]\n",
        "    db = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    error = predictions - y_one_hot\n",
        "    dW[-1] = np.dot(activations[-2].T, error) / m\n",
        "    db[-1] = np.sum(error, axis=0, keepdims=True) / m\n",
        "\n",
        "    for l in reversed(range(num_layers - 1)):\n",
        "        error = np.dot(error, weights[l+1].T) * activation_derivative(activations[l+1])\n",
        "        dW[l] = np.dot(activations[l].T, error) / m\n",
        "        db[l] = np.sum(error, axis=0, keepdims=True) / m\n",
        "\n",
        "    for l in range(num_layers):\n",
        "        weights[l] -= learning_rate * dW[l]\n",
        "        biases[l] -= learning_rate * db[l]\n",
        "\n",
        "    return weights, biases, loss\n",
        "\n"
      ],
      "metadata": {
        "id": "ul_JLofTklzc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_momentum(x_train, y_train, weights, biases, learning_rate=0.01,\n",
        "                        momentum=0.9, epochs=100, batch_size=None, activation='sigmoid'):\n",
        "\n",
        "    num_layers = len(weights)\n",
        "    m = x_train.shape[0]\n",
        "\n",
        "    velocity_w = [np.zeros_like(w) for w in weights]\n",
        "    velocity_b = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "    if activation == 'sigmoid':\n",
        "        activation_func = ActivationFunction.sigmoid\n",
        "        activation_derivative = ActivationFunction.sigmoid_derivative\n",
        "    elif activation == 'relu':\n",
        "        activation_func = ActivationFunction.relu\n",
        "        activation_derivative = ActivationFunction.relu_derivative\n",
        "    elif activation == 'tanh':\n",
        "        activation_func = ActivationFunction.tanh\n",
        "        activation_derivative = ActivationFunction.tanh_derivative\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    print(f\"  Training for {epochs} epochs with momentum = {momentum}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(m)\n",
        "        x_shuffled = x_train[indices]\n",
        "        y_shuffled = y_train[indices]\n",
        "\n",
        "        activations = [x_shuffled]\n",
        "        zs = []\n",
        "\n",
        "        for l in range(num_layers - 1):\n",
        "            z = np.dot(activations[-1], weights[l]) + biases[l]\n",
        "            zs.append(z)\n",
        "            a = activation_func(z)\n",
        "            activations.append(a)\n",
        "\n",
        "        z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
        "        zs.append(z)\n",
        "        predictions = ActivationFunction.softmax(z)\n",
        "        activations.append(predictions)\n",
        "\n",
        "        y_one_hot = np.zeros((m, weights[-1].shape[1]))\n",
        "        y_one_hot[np.arange(m), y_shuffled.astype(int)] = 1\n",
        "\n",
        "        loss = -np.sum(y_one_hot * np.log(predictions + 1e-8)) / m\n",
        "        losses.append(loss)\n",
        "\n",
        "        dW = [np.zeros_like(w) for w in weights]\n",
        "        db = [np.zeros_like(b) for b in biases]\n",
        "\n",
        "        error = predictions - y_one_hot\n",
        "        dW[-1] = np.dot(activations[-2].T, error) / m\n",
        "        db[-1] = np.sum(error, axis=0, keepdims=True) / m\n",
        "\n",
        "        for l in reversed(range(num_layers - 1)):\n",
        "            error = np.dot(error, weights[l+1].T) * activation_derivative(activations[l+1])\n",
        "            dW[l] = np.dot(activations[l].T, error) / m\n",
        "            db[l] = np.sum(error, axis=0, keepdims=True) / m\n",
        "\n",
        "        for l in range(num_layers):\n",
        "            velocity_w[l] = momentum * velocity_w[l] + learning_rate * dW[l]\n",
        "            velocity_b[l] = momentum * velocity_b[l] + learning_rate * db[l]\n",
        "            weights[l] -= velocity_w[l]\n",
        "            biases[l] -= velocity_b[l]\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"    Epoch {epoch:3d}: Loss = {loss:.4f}\")\n",
        "\n",
        "    return weights, biases, losses\n",
        "\n"
      ],
      "metadata": {
        "id": "Uf8A-HxCxEXu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5iEN0IPjzYx"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}